{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with the PAL algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's something interesting: https://github.com/VitoChan1/PAL-on-Adder-DSE/blob/master/PAL.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing the paper (PAL: An Active Learning Approach to the Multi-Objective Optimization Problem form the Puschel group at ETH)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* error bounds based on hypervolume error\n",
    "* response surface methods for unevaluated designs. Best so far: PareEGO also uses GP\n",
    "* scalariztion: without assumptions like convexity not all solutions can be recovered\n",
    "\n",
    "* uncertainity captured with hyperparameter, there is some scaling parameter beta that determines which fraction of the variance is used. \n",
    "* then classified as pareto optimal if the pessimistic bound is not dominated by the optimistic outcome of any other point\n",
    "* if the optimisitc bound is dominated by the pessimistic bound of any other point, then not pareto optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pareto_optimal(scores: np.array) -> np.array:\n",
    "    size = scores.shape[0]\n",
    "    ids = np.arange(size)\n",
    "    pareto_front = np.ones(size, dtype=bool)\n",
    "        for j in range(size):\n",
    "            # Check if our 'i' point is dominated by our 'j' point\n",
    "            if all(scores[j] >= scores[i]) and any(scores[j] > scores[i]):\n",
    "                # j dominates i. Label 'i' point as dominant\n",
    "                pareto_front[i] = 0\n",
    "                break\n",
    "    # Return ids of pareto front\n",
    "    return ids[pareto_front]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use PyGMO as they have spent more thought into how to implement this: \n",
    "\n",
    "https://esa.github.io/pygmo/documentation/hypervolume.html\n",
    "\n",
    "Implements, e.g., WFG (https://ieeexplore.ieee.org/document/5766730)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygmo as pg \n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypervolume(pareto_front: np.array, reference_vector: np.array) -> float: \n",
    "    hyp = pg.hypervolume(pareto_front) \n",
    "    volume = hyp.compute(reference_vector) # uses 'auto' algorithm\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_gp_predictions(gps: list, x_train: np.array, y_train: np.array, x_input: np.array) -> Union[np.array, np.array]: \n",
    "    # get the GP predictions, for generality, we will assume a list of GPs\n",
    "    # one GP per target\n",
    "    mus = []\n",
    "    stds = []\n",
    "    \n",
    "    for gp in gps: \n",
    "        gp.fit(x_train, y_train, normalize_y=True)\n",
    "        mu, std = gp.predict(x_input, return_std=True)\n",
    "        mus.append(mu)\n",
    "        stds.append(std)\n",
    "        \n",
    "    return np.hstack(mus), np.hstack(stds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_uncertainity_region(mu: float, std: float, beta_sqrt: float): \n",
    "    low_lim, high_lim = mu -  beta_sqrt * std, mu +   beta_sqrt * std\n",
    "    return low_lim, high_lim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_uncertainity_regions(mus: np.array, stds: np.array, beta_sqrt: float): \n",
    "    low_lims, high_lims = [], []\n",
    "    for i in range(0, mus.shape[1]):\n",
    "        low_lim, high_lim = _get_uncertainity_region(mus[:,i], stds[:,i], beta_sqrt)\n",
    "        low_lims.append(low_lim)\n",
    "        high_lims.append(high_lim)\n",
    "        \n",
    "    return np.hstack(low_lims), np.hstack(high_lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _union(lows: list, ups: list, new_lows: list, new_ups: list) -> Union[list, list]:\n",
    "    out_lows = []\n",
    "    out_ups = []\n",
    "    for i in range(0,len(l)):\n",
    "        if (new_lows[i] > ups[i]) or (new_ups[i] < low[i]) or (low[i] + ups[i] == 0):\n",
    "            out_lows.append(0)\n",
    "            out_ups.append(0)\n",
    "        else:\n",
    "            out_lows.append(max(lows[i], new_lows[i]))\n",
    "            out_ups.append(min(ups[i],new_ups[i]))\n",
    "    return out_lows, out_ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_sampled(mus, stds, sampled, y_input):\n",
    "    # this is kinda inefficient, better use some kind of indexing \n",
    "    for i in range(0,len(mus)):\n",
    "        if (sampled[i] == 1):\n",
    "            mus[i, :] = y_input[i, :]\n",
    "            \n",
    "            # ToDo: is this true? \n",
    "            stds[i, :] = 0\n",
    "            \n",
    "    return mus, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pareto_classify(pareto_optimal_0: list, not_pareto_optimal_0: list, unclassified_0: list, rectangle_lows: np.array, rectangle_ups: np.array, x_input: np.array, epsilon: float) -> Union[list, list, list]:\n",
    "    pareto_optimal_t = pareto_optimal_0\n",
    "    not_pareto_optimal_t = not_pareto_optimal_0\n",
    "    unclassified_t = unclassified_0\n",
    "    \n",
    "    # loop over samples \n",
    "    for i in range(0, x_input):\n",
    "        if (unclassified_t[i] == 1):\n",
    "            pareto = True\n",
    "            nonpareto = False\n",
    "            \n",
    "            # At iteration t, the points in Pt−1 and Nt−1 keep their classification. The only points x to be reclassified are those in Ut−1 , done as follows\n",
    "            \n",
    "            # If the pessimistic outcome min(Rt(x)) of x is not dominated by the optimistic outcome max(Rt(x )) of any other point (up to a shift of ε by both),\n",
    "            if all(rectangle_lows[i] * (1 + epsilon) <= rectangle_ups[i] * (1 - epsilon)):\n",
    "                pareto = False\n",
    "                \n",
    "            # If the optimistic outcome max(Rt(x)) of x is dominated by the pessimistic outcome min(Rt(x′)) of any x′ (up to a shift of ε by both),    \n",
    "            if all(rectangle_ups[i] * (1 - epsilon) <= rectangle_lows[i] * (1 + epsilon)): \n",
    "                nonpareto = True\n",
    "            \n",
    "            # All other points remain unclassified.\n",
    "            if pareto:\n",
    "                pareto_optimal_t[x] = 1\n",
    "                unclassified_t[x] = 0\n",
    "            elif nonpareto:\n",
    "                not_pareto_optimal_t[x] = 1\n",
    "                unclassified_t[x] = 0\n",
    "                \n",
    "    return pareto_optimal_t, not_pareto_optimal_t, unclassified_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample(rectangle_lows, rectangle_ups, pareto_optimal_t, non_pareto_optimal_t, unclassified_t, sampled, x_input, x_train):\n",
    "    maxwt = 0\n",
    "    maxid = -1\n",
    "    \n",
    "    for i in range(0,len(x_input)):\n",
    "        # Among the points x ∈ Pt ∪ Ut, the one with the largest wt(x) is chosen as the next sample xt to be evaluated.\n",
    "        # Intuitively, this rule biases the sampling towards exploring, and thus improving the model for, the points most likely to be Pareto-optimal.\n",
    "        if ((unclassified_t[x] == 1) or (pareto_optimal_t[x] == 1)) and not(sampled[x] == 1):\n",
    "            # weight is the length of the diagonal of the uncertainity region\n",
    "            wt = np.linalg.norm(rectangle_ups[i,:] - rectangle_lows[i,:])\n",
    "            if maxid == -1:\n",
    "                maxwt = wt\n",
    "                maxid = x\n",
    "            # the point with the largest weight is chosen as the next sample\n",
    "            elif wt > maxwt:\n",
    "                maxwt = wt\n",
    "                maxid = x\n",
    "                \n",
    "    x_train = np.insert(x_train, x_train.shape[0], x_input[maxid], axis = 0)\n",
    "    sampled[maxid] = 1\n",
    "    \n",
    "    return x_train, sampledle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pal(gps: list, x_train: np.array, y_train: np.array, x_input: np.array, y_input: np.array, delta: float = 0.05, epsilon: float = 0.001, iterations: int = 100):\n",
    "    # x_input is the E set in the PAL paper \n",
    "    # in the beginning, nothing is selected = everything is unclassified\n",
    "    pareto_optimal_0 = [0] * len(x_input)\n",
    "    not_pareto_optimal_0 = [0] * len(x_input)\n",
    "    unclassified_0 = [1] * len(x_input) \n",
    "    sampled = [0] * len(x_input)\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    # stop when all points are classified \n",
    "    while (np.sum(unclassified_0) > 0) and (iteration <= iterations):\n",
    "        # STEP 1: modeling (train and predict using GPR, one GP per target)\n",
    "        mus, stds = _get_gp_predictions(gps, x_train, y_train, x_input)\n",
    "    \n",
    "        # ToDo: check how beta is computed the heuristics depend on the kernel \n",
    "        # which is achieved by choosing βt = 2 log(n|E|π2t2/(6δ)).\n",
    "        # n: number of objectives (y_input.shape[1])\n",
    "        \n",
    "        beta = 2 * np.log(y_input.shape[1] * len(x) * np.square(np.pi) * np.square(iteration) / (6 * delta))\n",
    "            \n",
    "        # if point is sampled we know the mu and have no epistemic uncertainity    \n",
    "        mus, stds = _update_sampled(mus, stds, sampled, y_input)\n",
    "        \n",
    "        # get the uncertainity rectangles, sqrt only once here for efficiency\n",
    "        lows, ups = _get_uncertainity_regions(mus, stds, np.sqrt(beta))\n",
    "        \n",
    "        if iteration == 1: \n",
    "            # initialization\n",
    "            rectangle_lows, rectangle_ups = lows, ups\n",
    "        else:\n",
    "            rectangle_lows, rectangle_ups = _union(rectangle_lows, rectangle_ups, lows, ups)\n",
    "            \n",
    "            \n",
    "        # pareto classification, to be factored out\n",
    "        # update lists \n",
    "        pareto_optimal_t, not_pareto_optimal_t, unclassified_t = _pareto_classify(pareto_optimal_0, \n",
    "                                                                                  not_pareto_optimal_0, \n",
    "                                                                                  unclassified_0, \n",
    "                                                                                  rectangle_lows, \n",
    "                                                                                  rectangle_ups, x_input, epsilon)\n",
    "\n",
    "        \n",
    "        # sampling from x_input \n",
    "        x_train, sampled = _sample(rectangle_lows, rectangle_ups, pareto_optimal_t, non_pareto_optimal_t, unclassified_t, sampled, x_input, x_train)\n",
    "        pareto_optimal_0, not_pareto_optimal_0, unclassified_0 = pareto_optimal_t, not_pareto_optimal_t, unclassified_t\n",
    "        \n",
    "        \n",
    "    return pareto_optimal_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dispersant_basf] *",
   "language": "python",
   "name": "conda-env-dispersant_basf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
